import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')
import xgboost as xgb
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix, 
                             roc_auc_score, roc_curve, precision_recall_curve, f1_score)
from sklearn.preprocessing import LabelEncoder
 
 
df = pd.read_csv('/kaggle/input/noshowappointments/KaggleV2-May-2016.csv')
print("Dataset Shape:", df.shape)
print("\nFirst few rows:")
print(df.head())
print("\nColumn Info:")
print(df.info())
print("\nMissing Values:")
print(df.isnull().sum())
print("\nNo-show Distribution:")
print(df['No-show'].value_counts())
 
 
 
 
# fix col names 
df.rename(columns={
    'PatientId': 'patient_id',
    'AppointmentID': 'appointment_id',
    'Gender': 'gender',
    'ScheduledDay': 'scheduled_day',
    'AppointmentDay': 'appointment_day',
    'Age': 'age',
    'Neighbourhood': 'neighbourhood',
    'Scholarship': 'scholarship',
    'Hipertension': 'hypertension',
    'Diabetes': 'diabetes',
    'Alcoholism': 'alcoholism',
    'Handcap': 'handicap',
    'SMS_received': 'sms_received',
    'No-show': 'no_show'
}, inplace=True)
 
# convert datetime cols
df['scheduled_day'] = pd.to_datetime(df['scheduled_day'])
df['appointment_day'] = pd.to_datetime(df['appointment_day'])
 
# cleaning no_show var
df['no_show'] = df['no_show'].map({'No': 0, 'Yes': 1})
 
# invalid records - remove
df = df[(df['age'] >= 0) & (df['age'] <= 120)]
df = df[df['appointment_day'] >= df['scheduled_day']]
 
# remove duplicates
df = df.drop_duplicates(subset=['appointment_id'])
df = df.sort_values(['scheduled_day', 'appointment_day']).reset_index(drop=True)
 
print(f"\nCleaned Dataset Shape: {df.shape}")
print(f"No-show Rate: {df['no_show'].mean():.2%}")
 
 
 
# extracting time-based features
df['scheduled_date'] = df['scheduled_day'].dt.date
df['appointment_date'] = df['appointment_day'].dt.date
 
# hour block (morning/afternoon/evening)
df['scheduled_hour'] = df['scheduled_day'].dt.hour
df['appointment_hour'] = df['appointment_day'].dt.hour
def get_hour_block(hour):
    if hour < 8:
        return 0  # early morning
    elif hour < 12:
        return 1  # morning
    elif hour < 16:
        return 2  # afternoon
    else:
        return 3  # evening
df['hour_block'] = df['appointment_hour'].apply(get_hour_block)
 
df['day_of_week'] = df['appointment_day'].dt.dayofweek
 
df['is_holiday_or_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)
 
# calc lead time (days between scheduling and appointment)
df['lead_time_days'] = (df['appointment_day'] - df['scheduled_day']).dt.days
df['lead_time_hours'] = (df['appointment_day'] - df['scheduled_day']).dt.total_seconds() / 3600
 
df['same_day_appointment'] = (df['lead_time_days'] == 0).astype(int)
 
df['appointment_month'] = df['appointment_day'].dt.month
df['appointment_week'] = df['appointment_day'].dt.isocalendar().week
 
print("Time-based features created successfully")
 
 
 
 
 
df = df.sort_values(['patient_id', 'appointment_day']).reset_index(drop=True)
 
patient_history = df.groupby('patient_id').agg({
    'no_show': ['sum', 'count', 'mean'],  # Total no-shows, total appointments, no-show rate
    'age': 'first',
    'gender': 'first'
}).reset_index()
 
patient_history.columns = ['patient_id', 'total_no_shows', 'total_appointments', 
                           'historical_no_show_rate', 'age', 'gender']
 
df = df.merge(patient_history[['patient_id', 'total_no_shows', 'total_appointments', 
                                'historical_no_show_rate']], 
              on='patient_id', how='left')
 
df['prev_no_shows'] = df.groupby('patient_id')['no_show'].cumsum() - df['no_show']
df['prev_appointments'] = df.groupby('patient_id').cumcount()
df['rolling_no_show_rate'] = np.where(
    df['prev_appointments'] > 0,
    df['prev_no_shows'] / df['prev_appointments'],
    df['historical_no_show_rate']  
)
 
overall_no_show_rate = df['no_show'].mean()
df['rolling_no_show_rate'] = df['rolling_no_show_rate'].fillna(overall_no_show_rate)
 
print("Patient historical features created successfully")
 
 
# avg historical no-show rate by hour block
hourly_no_show = df.groupby('hour_block')['no_show'].mean().reset_index()
hourly_no_show.columns = ['hour_block', 'avg_historical_no_show_rate_by_hour']
df = df.merge(hourly_no_show, on='hour_block', how='left')
 
# avg no-show rate by day of week
dow_no_show = df.groupby('day_of_week')['no_show'].mean().reset_index()
dow_no_show.columns = ['day_of_week', 'avg_no_show_rate_by_dow']
df = df.merge(dow_no_show, on='day_of_week', how='left')
 
# avg no-show rate by neighbourhood
neighbourhood_no_show = df.groupby('neighbourhood')['no_show'].mean().reset_index()
neighbourhood_no_show.columns = ['neighbourhood', 'avg_no_show_rate_by_neighbourhood']
df = df.merge(neighbourhood_no_show, on='neighbourhood', how='left')
 
# age group no-show patterns
df['age_group'] = pd.cut(df['age'], bins=[0, 18, 35, 50, 65, 120], labels=['0-18', '19-35', '36-50', '51-65', '65+'])
age_no_show = df.groupby('age_group')['no_show'].mean().reset_index()
age_no_show.columns = ['age_group', 'avg_no_show_rate_by_age']
df = df.merge(age_no_show, on='age_group', how='left')
 
print("Aggregated historical pattern features created successfully")
 
 
le_gender = LabelEncoder()
df['gender_encoded'] = le_gender.fit_transform(df['gender'])
le_neighbourhood = LabelEncoder()
df['neighbourhood_encoded'] = le_neighbourhood.fit_transform(df['neighbourhood'])
 
# select features for modeling
feature_columns = [
    'hour_block', 'day_of_week', 'is_holiday_or_weekend', 'lead_time_days', 
    'same_day_appointment', 'appointment_month',
    'age', 'gender_encoded', 'scholarship', 'hypertension', 'diabetes', 
    'alcoholism', 'handicap', 'sms_received',
    'rolling_no_show_rate', 'prev_appointments',
    'avg_historical_no_show_rate_by_hour', 'avg_no_show_rate_by_dow',
    'avg_no_show_rate_by_neighbourhood', 'avg_no_show_rate_by_age'
]
 
X = df[feature_columns].copy()
y = df['no_show'].copy()
X = X.fillna(X.mean())
 
print(f"\nFinal Feature Matrix Shape: {X.shape}")
print(f"Target Distribution:\n{y.value_counts(normalize=True)}")
print(f"\nFeatures used: {len(feature_columns)}")
 
 
# split data 
split_index = int(len(df) * 0.8)
 
X_train = X.iloc[:split_index]
X_test = X.iloc[split_index:]
y_train = y.iloc[:split_index]
y_test = y.iloc[split_index:]
 
X_train_final, X_val, y_train_final, y_val = train_test_split(
    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train
)
print(f"Training set: {X_train_final.shape}, No-show rate: {y_train_final.mean():.2%}")
print(f"Validation set: {X_val.shape}, No-show rate: {y_val.mean():.2%}")
print(f"Test set: {X_test.shape}, No-show rate: {y_test.mean():.2%}")
 
 
# calculate scale_pos_weight (imbalanced data)
no_show_ratio = (y_train_final == 0).sum() / (y_train_final == 1).sum()
print(f"\nClass imbalance ratio (No-show=0 / No-show=1): {no_show_ratio:.2f}")
print(f"This will be used as scale_pos_weight in XGBoost")
 
 
# initial XGBoost model 
xgb_model = xgb.XGBClassifier(
    n_estimators=300,
    max_depth=6,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    scale_pos_weight=no_show_ratio,  
    random_state=42,
    eval_metric='auc',
    early_stopping_rounds=20,
    tree_method='hist',  
    n_jobs=-1
)
 
# Train model 
print("\nTraining XGBoost model...")
xgb_model.fit(
    X_train_final, y_train_final,
    eval_set=[(X_val, y_val)],
    verbose=50
)
 
print("\nModel training completed!")
 
 
# parameter grid - tuning
param_grid = {
    'max_depth': [4, 6, 8],
    'learning_rate': [0.01, 0.05, 0.1],
    'n_estimators': [200, 300, 400],
    'subsample': [0.7, 0.8, 0.9],
    'colsample_bytree': [0.7, 0.8, 0.9],
    'min_child_weight': [1, 3, 5]
}
 
# initialize GridSearchCV
print("\nStarting hyperparameter tuning...")
grid_search = GridSearchCV(
    estimator=xgb.XGBClassifier(
        scale_pos_weight=no_show_ratio,
        random_state=42,
        eval_metric='auc',
        tree_method='hist',
        n_jobs=-1
    ),
    param_grid=param_grid,
    scoring='roc_auc',
    cv=3,
    verbose=2,
    n_jobs=-1
)
 
# grid search
grid_search.fit(X_train_final, y_train_final)
 
# best parameters
print(f"\nBest parameters: {grid_search.best_params_}")
print(f"Best cross-validation AUC: {grid_search.best_score_:.4f}")
 
# best model
best_xgb_model = grid_search.best_estimator_
 
 
 
 
 
 
#predictions
y_train_pred = best_xgb_model.predict(X_train_final)
y_val_pred = best_xgb_model.predict(X_val)
y_test_pred = best_xgb_model.predict(X_test)
 
y_train_pred_proba = best_xgb_model.predict_proba(X_train_final)[:, 1]
y_val_pred_proba = best_xgb_model.predict_proba(X_val)[:, 1]
y_test_pred_proba = best_xgb_model.predict_proba(X_test)[:, 1]
 
#metrics
print("\n" + "="*60)
print("MODEL PERFORMANCE METRICS")
print("="*60)
 
for name, y_true, y_pred, y_pred_proba in [
    ("Training", y_train_final, y_train_pred, y_train_pred_proba),
    ("Validation", y_val, y_val_pred, y_val_pred_proba),
    ("Test", y_test, y_test_pred, y_test_pred_proba)
]:
    accuracy = accuracy_score(y_true, y_pred)
    roc_auc = roc_auc_score(y_true, y_pred_proba)
    f1 = f1_score(y_true, y_pred)
    
    print(f"\n{name} Set:")
    print(f"  Accuracy:  {accuracy:.4f}")
    print(f"  ROC-AUC:   {roc_auc:.4f}")
    print(f"  F1-Score:  {f1:.4f}")
 
print("\n" + "="*60)
print("DETAILED CLASSIFICATION REPORT (Test Set)")
print("="*60)
print(classification_report(y_test, y_test_pred, target_names=['Attended', 'No-Show']))